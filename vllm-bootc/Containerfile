# =============================================================================
# Stage 1: Builder – UBI9, build venv with vLLM + deps (NO SUBSCRIPTION REQUIRED)
# =============================================================================
FROM registry.access.redhat.com/ubi9/ubi:latest AS builder

ARG PYTHON_VERSION=3.9
ARG VLLM_VERSION=0.10.2

# Build-time deps only (stay in builder image)
# UBI9 has Python 3.9 by default - NO SUBSCRIPTION REQUIRED
# Install build tools needed for vLLM source build (for both architectures)
RUN dnf -y install \
      python3 \
      python3-pip \
      python3-devel \
      gcc gcc-c++ \
      cmake ninja-build \
      make \
      git jq which procps-ng \
      numactl-devel \
    && dnf clean all
  
# Create vLLM venv using Python 3.9
RUN python3 -m venv /opt/vllm-venv

ENV PATH="/opt/vllm-venv/bin:${PATH}" \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    PYTHONUNBUFFERED=1

# Upgrade pip tooling
RUN pip install --no-cache-dir --upgrade pip setuptools wheel

# CPU/GPU-aware PyTorch install:
# - aarch64 → CPU wheels
# - x86_64  → CUDA 12.1 wheels
RUN if [ "$(uname -m)" = "aarch64" ]; then \
      pip install --no-cache-dir \
        torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cpu ; \
    else \
      pip install --no-cache-dir \
        torch torchvision torchaudio \
        --index-url https://download.pytorch.org/whl/cu121 ; \
    fi

# vLLM installation: Build from source for both architectures (NO SUBSCRIPTION REQUIRED)
# Build from source for consistency and to avoid subscription requirements
# Both architectures build with CPU support (can use CUDA at runtime if available)
RUN git clone --depth 1 --branch v${VLLM_VERSION} https://github.com/vllm-project/vllm.git /tmp/vllm-src || \
    (echo "Branch v${VLLM_VERSION} not found, using main branch..." && \
     git clone --depth 1 https://github.com/vllm-project/vllm.git /tmp/vllm-src) && \
    cd /tmp/vllm-src && \
    pip install --no-cache-dir -r requirements/build.txt && \
    pip install --no-cache-dir -r requirements/cpu.txt && \
    # Handle NUMA: remove -lnuma from CMakeLists.txt and create stub if needed
    # This works for both architectures and prevents linker errors
    find . -type f \( -name "*.py" -o -name "CMakeLists.txt" -o -name "*.cmake" \) -exec sed -i 's/-lnuma//g' {} + 2>/dev/null || true && \
    mkdir -p /tmp/numa_stub && \
    printf '#include <stdlib.h>\nint numa_available(void) { return -1; }\nvoid* numa_alloc_onnode(size_t size, int node) { return malloc(size); }\nvoid numa_free(void *start, size_t size) { free(start); }\nint numa_node_of_cpu(int cpu) { return 0; }\nvoid numa_run_on_node(int node) {}\n' > /tmp/numa_stub/numa_stub.c && \
    gcc -shared -fPIC -o /tmp/numa_stub/libnuma.so /tmp/numa_stub/numa_stub.c 2>/dev/null || true && \
    CC=/usr/bin/gcc \
    CXX=/usr/bin/g++ \
    CXXFLAGS="-Wno-error -Wno-psabi -DVLLM_NUMA_DISABLED" \
    LDFLAGS="-Wl,--as-needed -L/tmp/numa_stub" \
    CMAKE_BUILD_PARALLEL_LEVEL=1 \
    MAX_JOBS=1 \
    SETUPTOOLS_SCM_PRETEND_VERSION=${VLLM_VERSION} \
    VLLM_TARGET_DEVICE=cpu \
    pip install --no-cache-dir . --no-build-isolation && \
    rm -rf /tmp/vllm-src /tmp/numa_stub

# Runtime deps (common for both architectures)
RUN pip install --no-cache-dir \
      fastapi \
      "uvicorn[standard]" \
      transformers \
      sentencepiece \
      huggingface_hub


# =============================================================================
# Stage 2: Runtime – RHEL 9 bootc base (NO SUBSCRIPTION REQUIRED)
# =============================================================================
# Using RHEL 9 bootc base image - bootc tools are pre-installed
# This is a proper bootc base image for creating bootable containers
FROM registry.redhat.io/rhel9/rhel-bootc:latest

ARG PYTHON_VERSION=3.9

# RHEL 9 bootc base has Python 3.9 and bootc tools pre-installed
# Note: systemd-sysusers is included with systemd package, not a separate package
# Ensure Python 3.9 is available (should be pre-installed in bootc base)
RUN dnf -y install \
      python3 \
      systemd \
    && dnf clean all

# Copy pre-built venv into final image
COPY --from=builder /opt/vllm-venv /opt/vllm-venv

# Fix venv symlinks to point to the correct Python 3.9 location
# Both builder and runtime use Python 3.9 from system repos, so symlinks should work
# But verify and fix if needed
RUN if [ ! -f /opt/vllm-venv/bin/python3.9 ] || [ ! -x /opt/vllm-venv/bin/python3.9 ]; then \
      rm -f /opt/vllm-venv/bin/python3.9 /opt/vllm-venv/bin/python3 /opt/vllm-venv/bin/python && \
      ln -sf /usr/bin/python3.9 /opt/vllm-venv/bin/python3.9 && \
      ln -sf /usr/bin/python3.9 /opt/vllm-venv/bin/python3 && \
      ln -sf /usr/bin/python3.9 /opt/vllm-venv/bin/python; \
    fi


# Optionally make venv first on PATH (nice but not strictly required)
ENV PATH="/opt/vllm-venv/bin:${PATH}" \
    PYTHONUNBUFFERED=1

# Copy /etc contents in one go (sysconfig, systemd units, sysusers)
COPY etc/ /etc/

# Initializer script
COPY vllm/initializer-entrypoint.sh /usr/local/bin/initializer-entrypoint.sh

# For arm64 builds, add CPU-only environment variables to systemd service
# This ensures they're set before Python starts, preventing CUDA library loading
# Since we built from source with VLLM_TARGET_DEVICE=cpu, CUDA libraries won't be needed
RUN ARCH=$(uname -m) && \
    if [ "$ARCH" = "aarch64" ]; then \
        # Insert CPU-only env vars in [Service] section before ExecStart
        awk '/^ExecStart=/ { \
            print "# CPU-only mode for arm64 (set before Python starts to prevent CUDA loading)"; \
            print "Environment=CUDA_VISIBLE_DEVICES="; \
            print "Environment=VLLM_NO_CUDA=1"; \
            print "Environment=VLLM_CPU_ONLY=1"; \
            print "Environment=VLLM_PLATFORM=cpu"; \
            print "Environment=VLLM_SKIP_PLATFORM_CHECK=1"; \
            print "Environment=VLLM_USE_FLASHINFER=0"; \
        } 1' /etc/systemd/system/rhoim-vllm.service > /tmp/rhoim-vllm.service.tmp && \
        mv /tmp/rhoim-vllm.service.tmp /etc/systemd/system/rhoim-vllm.service; \
    fi

RUN chmod +x /usr/local/bin/initializer-entrypoint.sh \
    # Create rhoim user via systemd-sysusers
    && systemd-sysusers \
    # Create RHOIM layout and assign ownership to rhoim
    && mkdir -p /opt/rhoim/{bin,models,gateway} /tmp/models \
    && chown -R rhoim:rhoim /opt/rhoim /tmp/models \
    # Create /sysroot directory required by bootc lint
    && mkdir -p /sysroot \
    # Enable the vLLM service
    && systemctl enable rhoim-vllm.service

# vLLM listens here (still helpful when running as a plain container)
EXPOSE 8000

# Mark as bootc-compatible
LABEL org.osbuild.bootc=true \
      bootc=true

# Final bootc lint step (non-fatal - UBI9 may not pass all checks, but image is bootc-compatible)
RUN bootc container lint || echo "Bootc lint completed with warnings (expected for UBI9 base)"

# Bootc images must run systemd as PID 1
CMD ["/sbin/init"]